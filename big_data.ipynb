{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meryembouaddi/challenge-bigdata/blob/main/big_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1359ee44",
      "metadata": {
        "id": "1359ee44"
      },
      "source": [
        "# DATA INGESTION des commentaires de HESPRESS et le stockage dans Mongo db  :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2f5df3d",
      "metadata": {
        "id": "d2f5df3d"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# Define the URL to scrape\n",
        "url=\"https://www.hespress.com/%d9%88%d9%81%d8%a7%d8%a9-4-%d9%85%d8%ba%d8%a7%d8%b1%d8%a8%d8%a9-%d8%a5%d8%ab%d8%b1-%d8%a7%d9%84%d8%b2%d9%84%d8%b2%d8%a7%d9%84-%d8%a7%d9%84%d8%b9%d9%86%d9%8a%d9%81-%d8%a8%d8%aa%d8%b1%d9%83%d9%8a%d8%a7-1120395.html\"# Make a GET request to the website\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML using BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Extract the comments from the HTML\n",
        "comments = soup.find_all(\"div\", class_=\"comments\")\n",
        "\n",
        "# Connect to the MongoDB instance\n",
        "client = MongoClient(\"mongodb+srv://meryembou:<1234>@cluster0.ryfnnk2.mongodb.net/?retryWrites=true&w=majority\")\n",
        "\n",
        "# Get the comments collection\n",
        "comments_collection = client.test.comments\n",
        "\n",
        "comments_to_insert = []\n",
        "# Iterate through the comments and extract the text of the two div elements\n",
        "for comment in comments:\n",
        "    for ul in comment.find_all('ul'):\n",
        "        for li in ul.find_all('li'):\n",
        "            div_container = li.find('div', {'class': 'comment-body'})\n",
        "            div1 = div_container.find('div', {'class': 'comment-head'})\n",
        "            div2 = div_container.find('div', {'class': 'comment-text'})\n",
        "            comment_obj = {\"author\": div1.text.split(\"\\n\")[2], \"text\": div2.text}\n",
        "            comments_to_insert.append(comment_obj)\n",
        "\n",
        "# insert all comments\n",
        "#comments_collection.insert_many(comments_to_insert)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb9c8da7",
      "metadata": {
        "id": "bb9c8da7"
      },
      "outputs": [],
      "source": [
        "#Data ingestion avec un topic  \"comments\"\n",
        "\n",
        "from confluent_kafka import Producer, Consumer\n",
        "import json\n",
        "\n",
        "# Kafka producer\n",
        "producer = Producer({'bootstrap.servers': 'localhost:9092'})\n",
        "\n",
        "# Iterate through the comments and extract the text of the two div elements\n",
        "for comment in comments:\n",
        "    for ul in comment.find_all('ul'):\n",
        "        for li in ul.find_all('li'):\n",
        "            div_container = li.find('div', {'class': 'comment-body'})\n",
        "            div1 = div_container.find('div', {'class': 'comment-head'})\n",
        "            div2 = div_container.find('div', {'class': 'comment-text'})\n",
        "            comment_obj = {\"author\": div1.text.split(\"\\n\")[2], \"text\": div2.text}\n",
        "            comment_json = json.dumps(comment_obj)\n",
        "            producer.produce('comments', value=bytes(comment_json, 'utf-8'))\n",
        "            producer.flush()\n",
        "            print(f'Published message: {comment_json}')\n",
        "\n",
        "\n",
        "# Kafka consumer\n",
        "conf = {'bootstrap.servers': 'localhost:9092',\n",
        "        'group.id': 'mygroup',\n",
        "        'auto.offset.reset': 'earliest'}\n",
        "\n",
        "consumer = Consumer(conf)\n",
        "\n",
        "consumer.subscribe(['comments'])\n",
        "\n",
        "\n",
        "\n",
        "while True:\n",
        "    msg = consumer.poll(5.0)\n",
        "    if msg is None:\n",
        "        continue\n",
        "    if msg.error():\n",
        "        if msg.error().code() == KafkaError._PARTITION_EOF:\n",
        "            print('Reached end of topic {} [{}] at offset {}'.format(\n",
        "                msg.topic(), msg.partition(), msg.offset()))\n",
        "        else:\n",
        "            print('Error occured: {}'.format(msg.error()))\n",
        "    else:\n",
        "        comment = json.loads(msg.value())\n",
        "        print(comment)\n",
        "consumer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ae1390a",
      "metadata": {
        "id": "4ae1390a",
        "outputId": "cbbfb048-df0d-4364-e4f6-acd0605efe68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting confluent_kafka\n",
            "  Downloading confluent_kafka-2.0.2-cp39-cp39-win_amd64.whl (3.3 MB)\n",
            "     ---------------------------------------- 3.3/3.3 MB 1.1 MB/s eta 0:00:00\n",
            "Installing collected packages: confluent_kafka\n",
            "Successfully installed confluent_kafka-2.0.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install confluent_kafka"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0456c770",
      "metadata": {
        "id": "0456c770",
        "outputId": "f8ea472f-9077-4de9-d587-e7250302d9ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5280\\508570821.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconsumer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from confluent_kafka import Producer, Consumer\n",
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# download the necessary NLTK resources\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Create a SentimentIntensityAnalyzer object\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Iterate through the comments and extract the text of the two div elements\n",
        "for comment in comments:\n",
        "    for ul in comment.find_all('ul'):\n",
        "        for li in ul.find_all('li'):\n",
        "            div_container = li.find('div', {'class': 'comment-body'})\n",
        "            div1 = div_container.find('div', {'class': 'comment-head'})\n",
        "            div2 = div_container.find('div', {'class': 'comment-text'})\n",
        "            comment_obj = {\"author\": div1.text, \"text\": div2.text}\n",
        "            comment_text = comment_obj[\"text\"]\n",
        "            sentiment = sia.polarity_scores(comment_text)\n",
        "            print(\"Comment text: \", comment_text)\n",
        "            print(\"Sentiment: \", sentiment)\n",
        "# Kafka consumer\n",
        "conf = {'bootstrap.servers': 'localhost:9092',\n",
        "        'group.id': 'mygroup',\n",
        "        'auto.offset.reset': 'earliest'}\n",
        "\n",
        "consumer = Consumer(conf)\n",
        "\n",
        "consumer.subscribe(['comments'])\n",
        "\n",
        "while True:\n",
        "    msg = consumer.poll(5.0)\n",
        "    if msg is None:\n",
        "        continue\n",
        "    if msg.error():\n",
        "        if msg.error().code() == KafkaError._PARTITION_EOF:\n",
        "            print('Reached end of topic {} [{}] at offset {}'.format(\n",
        "                msg.topic(), msg.partition(), msg.offset()))\n",
        "        else:\n",
        "            print('Error occured: {}'.format(msg.error()))\n",
        "    else:\n",
        "        comment = json.loads(msg.value())\n",
        "        print(comment)\n",
        "# close the consumer after the loop\n",
        "consumer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "039c2f53",
      "metadata": {
        "id": "039c2f53",
        "outputId": "eb75c615-41c9-42b0-cea3-8c9a86f726aa"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'textblob'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5280\\3820461159.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcomment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcomments\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mul\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcomment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ul'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mli\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mul\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'li'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "for comment in comments:\n",
        "    for ul in comment.find_all('ul'):\n",
        "        for li in ul.find_all('li'):\n",
        "            div_container = li.find('div', {'class': 'comment-body'})\n",
        "            div2 = div_container.find('div', {'class': 'comment-text'})\n",
        "            comment_text = div2.text\n",
        "            blob = TextBlob(comment_text)\n",
        "            sentiment = blob.sentiment\n",
        "            print(sentiment)\n",
        "while True:\n",
        "    msg = consumer.poll(1.0)\n",
        "    if msg is None:\n",
        "        continue\n",
        "    if msg.error():\n",
        "        if msg.error().code() == KafkaError._PARTITION_EOF:\n",
        "            print('Reached end of topic {} [{}] at offset {}'.format(\n",
        "                msg.topic(), msg.partition(), msg.offset()))\n",
        "        else:\n",
        "            print('Error occured: {}'.format(msg.error()))\n",
        "    else:\n",
        "        comment = json.loads(msg.value())\n",
        "        print(comment)\n",
        "# close the consumer after the loop\n",
        "consumer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db531783",
      "metadata": {
        "id": "db531783",
        "outputId": "b7820916-0a8a-48ed-e9c3-cb4a870f38ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting textblob\n",
            "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
            "     ------------------------------------ 636.8/636.8 kB 626.5 kB/s eta 0:00:00\n",
            "Requirement already satisfied: nltk>=3.1 in c:\\programdata\\anaconda\\lib\\site-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: tqdm in c:\\programdata\\anaconda\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
            "Requirement already satisfied: joblib in c:\\programdata\\anaconda\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
            "Requirement already satisfied: click in c:\\programdata\\anaconda\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.5)\n",
            "Installing collected packages: textblob\n",
            "Successfully installed textblob-0.17.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install textblob"
      ]
    },
    {
      "cell_type": "raw",
      "id": "573c526c",
      "metadata": {
        "id": "573c526c"
      },
      "source": [
        "apache spark "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5e7fbb8",
      "metadata": {
        "id": "d5e7fbb8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff9fbc7d",
      "metadata": {
        "id": "ff9fbc7d",
        "outputId": "65ab6e7e-7827-43eb-fadc-8c93047043cc"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Java gateway process exited before sending its port number",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5280\\4046957733.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Create Spark session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"BatchProcessing\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Load data from the local JSON file into a DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                     \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m                     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    193\u001b[0m             )\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             self._do_init(\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"BatchProcessing\").getOrCreate()\n",
        "\n",
        "# Load data from the local JSON file into a DataFrame\n",
        "df = spark.read.option(\"multiline\",\"true\").json(r\"C:\\Users\\hp\\Desktop\\projet big data\\comments.json\")\n",
        "\n",
        "# Perform batch processing on the data\n",
        "result = df.rdd\\\n",
        "    .map(lambda x: (x.author, x.text))\\\n",
        "    .groupByKey()\\\n",
        "    .mapValues(list)\\\n",
        "    .collect()\n",
        "\n",
        "# Show result\n",
        "for author, text in result:\n",
        "    print(f\"Author: {author}\\nText: {text}\\n\")\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66d34921",
      "metadata": {
        "id": "66d34921",
        "outputId": "710a058a-cd61-4686-e88b-7566d58e9648"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Java gateway process exited before sending its port number",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5280\\3948921970.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Create Spark session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"BatchProcessing\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Load data from the local JSON file into a DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                     \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m                     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    193\u001b[0m             )\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             self._do_init(\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"BatchProcessing\").getOrCreate()\n",
        "\n",
        "# Load data from the local JSON file into a DataFrame\n",
        "df = spark.read.option(\"multiline\",\"true\").json(r\"C:\\Users\\hp\\Desktop\\projet big data\\comments.json\")\n",
        "\n",
        "# Specify the batch size for processing\n",
        "batch_size = 82\n",
        "\n",
        "# Get the number of records in the DataFrame\n",
        "num_records = df.count()\n",
        "print(num_records)\n",
        "# Calculate the number of batches to process\n",
        "num_batches = (num_records + batch_size - 1) // batch_size\n",
        "print(num_batches)\n",
        "\n",
        "# Loop through the batches and process the data\n",
        "for i in range(num_batches):\n",
        "    # Get the start and end index for the current batch\n",
        "    start_index = i * batch_size\n",
        "    end_index = min(start_index + batch_size, num_records)\n",
        "    \n",
        "    # Get the DataFrame for the current batch\n",
        "    batch_df = df.limit(batch_size).coalesce(1)\n",
        "    \n",
        "    \n",
        "    # Perform batch processing on the data\n",
        "    result = batch_df.rdd\\\n",
        "        .map(lambda x: (x.author, x.text))\\\n",
        "        .groupByKey()\\\n",
        "        .mapValues(list)\\\n",
        "        .collect()\n",
        "    \n",
        "    # Show result\n",
        "    for author, text in result:\n",
        "        print(f\"Author: {author}\\nText: {text}\\n\")\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cea6967",
      "metadata": {
        "id": "1cea6967"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"BatchProcessing\").getOrCreate()\n",
        "\n",
        "# Load data from the local JSON file into a DataFrame\n",
        "df = spark.read.option(\"multiline\",\"true\").json(r\"C:\\Users\\hp\\Desktop\\projet big data\\comments.json\")\n",
        "\n",
        "# Perform batch processing on the data\n",
        "result = df.rdd\\\n",
        "    .map(lambda x: (x.author, x.text))\\\n",
        "    .groupByKey()\\\n",
        "    .mapValues(list)\\\n",
        "    .collect()\n",
        "\n",
        "# Perform sentiment analysis using TextBlob\n",
        "for author, text in result:\n",
        "    sentiment_scores = [TextBlob(t).sentiment.polarity for t in text]\n",
        "    average_sentiment = sum(sentiment_scores) / len(sentiment_scores)\n",
        "    if average_sentiment != 0:\n",
        "        print(f\"Author: {author}\\nAverage sentiment: {average_sentiment}\\n\")\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2022a215",
      "metadata": {
        "id": "2022a215"
      },
      "outputs": [],
      "source": [
        "from operator import add\n",
        "import re\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, explode, desc\n",
        "\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"BatchProcessing\").getOrCreate()\n",
        "\n",
        "# Load data from the local JSON file into a DataFrame\n",
        "df = spark.read.option(\"multiline\",\"true\").json(r\"C:\\Users\\hp\\Desktop\\projet big data\\comments.json\")\n",
        "\n",
        "# Split the \"text\" field into individual words\n",
        "df = df.select(\"author\", explode(split(df[\"text\"], \" \")).alias(\"word\"))\n",
        "\n",
        "# Group by word and count occurrences\n",
        "word_counts = df.groupBy(\"word\").count().sort(desc(\"count\"))\n",
        "\n",
        "# Show the most commonly used words\n",
        "word_counts.show(10)\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}